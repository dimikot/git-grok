#!/usr/bin/env python3
from __future__ import annotations
import argparse
import fcntl
import hashlib
import json
import os
import random
import re
import shlex
import signal
import ssl
import subprocess
import sys
import traceback
from dataclasses import dataclass
from datetime import datetime
from itertools import dropwhile
from subprocess import CalledProcessError, Popen, check_output
from threading import Thread, current_thread, main_thread
from time import time, sleep
from typing import Any, Callable, Literal, TypeVar, Generic, cast
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError

BRANCH_PREFIX = "grok/"
PR_HEADER = "Pull Request"
PR_HEADER_RE = rf"^\s+{PR_HEADER}: (https://\S+)(?: \((.*?)\))?"
MIDDLE_PR_LABEL = "git-grok-middle-pr"
MIDDLE_PR_LABEL_DESCRIPTION = "This PR is in the middle of the stack."
MIDDLE_PR_LABEL_COLOR = "CCCCCC"
BODY_SUFFIX_TITLE = "## PRs in the Stack"
BODY_SUFFIX_FOOTER = (
    "(The stack is managed by [git-grok](https://github.com/dimikot/git-grok).)"
)
BODY_SUFFIX_RE = (
    rf"^ {re.escape(BODY_SUFFIX_TITLE)} [^\n]* \n"
    + r"( (\s*(\n|\Z))* (- \s* [^\n]+ (\n|\Z)) )*"
    + r"( (\s*(\n|\Z))* [^\n]+ \bgit-grok\b [^\n]+ (\n|\Z) )?"
)
MIN_BRANCH_LEN_IN_PRINT = 36
MIN_TITLE_LEN_IN_PRINT = 36
INTERNAL_ENV_VAR_PREFIX = "GIT_GROK"
INTERNAL_IN_REBASE_INTERACTIVE_VAR = f"{INTERNAL_ENV_VAR_PREFIX}_in_rebase_interactive"
INTERNAL_SKIP_UPDATE_EXISTING_PRS_VAR = (
    f"{INTERNAL_ENV_VAR_PREFIX}_skip_update_existing_prs"
)
SETTINGS_FILE = ".git/git-grok.json"
PR_TEMPLATE_FILE = ".github/pull_request_template.md"
TMP_BODY_FILE = "/tmp/git-grok.body"
DEBUG_FILE = "/tmp/git-grok.log"
DEBUG_COLOR_GRAY_1 = (128, 128, 128)
DEBUG_COLOR_GRAY_2 = (92, 92, 92)
GH_MIN_VERSION = "2.29.0"
AI_MAX_REMEMBERED_SNIPPETS = 50
AI_PLACEHOLDER = "<!--AI-->"
AI_SEPARATOR = "=" * 20
AI_DIFF_CONTEXT_LINES = 100
AI_WHO_YOU_ARE = """
    You are a senior software engineer who writes professional, concise, and
    informative pull request descriptions.
"""
AI_PROMPT_INJECT_PLACEHOLDER = f"""
    # INSTRUCTIONS

    I am passing you the Pull Request Template text. Modify it according to the
    instructions below and return the text back to me.

    - It must be the exact text of the Pull Request Template that I passed in
      the input, BUT with "{AI_PLACEHOLDER}" marker injected on a separate line
      at the place where it will make sense for me to later insert the Pull
      Request Summary.
    - Your goal is to detect the single best place for this "{AI_PLACEHOLDER}"
      marker injection and inject it there. At the place where a human-readable
      summary would be expected.
    - You are NOT ALLOWED to remove a single character in the PR template!
    - You are NOT ALLOWED to add any new lines to the PR template EXCEPT that
      "{AI_PLACEHOLDER}" marker! I.e. you must always add exactly one new line
      with that marker, not more.
    - Never wrap anything with markdown blocks or something like that. Follow
      the rules above!
    - Never change sub-headers, surrounding text etc. there.
    - Do not delete any sections, even if those sections appear empty.
    - Only include the response text in that part. Don't add any comments or
      thoughts, just the text of the PR template with the "{AI_PLACEHOLDER}"
      marker injected at the best place.

    # EXAMPLE 1

    Input:

        ## Summary

        ## How was this tested?

    Output I expect from you:

        ## Summary

        {AI_PLACEHOLDER}

        ## How was this tested?

    # EXAMPLE 2

    Input:

        Overview

        Something else

    Output I expect from you:

        Overview

        {AI_PLACEHOLDER}

        Something else
"""
AI_PROMPT_GENERATE_SUMMARY = f"""
    I am passing you a Pull Request Title a the code diff. Generate the Pull
    Request Summary according to the following instructions.

    Guidelines for all of the texts you generate:

    - NEVER USE THE EXAGGERATION WORDS LIKE "CRITICAL", "CRUCIAL", "SIGNIFICANT"
      AND ANY OTHER WORDS WITH SIMILAR MEANING. Avoid pompous phrases like "This
      is a crucial step", "This change is foundational", "This is a significant
      enhancement" etc. Overall, be humble, don't try to judge the PR change and
      its importance; just provide the dry facts.
    - DON'T HALLUCINATE! If you're not sure about something, better not mention
      it than hallucinate. Also, do not say "likely" - avoid guessing! If
      unsure, try to infer hints from the PR title.

    # Top Part: Summary

    - Generate a concise, informative, and professional Pull Request Summary.
      When building, infer the information from the Pull Request Title and the
      diff provided.
    - Do not generate "Summary" sub-header or any other sub-headers. Instead,
      just generate the text (one or multiple paragraphs).
    - Don't be too short, but also don't be too verbose: the result should be
      pleasant for a human engineer to read and understand.
    - Use PR title; it is provided as a hint for the PRIMARY ESSENCE of the
      change in the diff (especially when unsure, or when there are multiple
      unrelated changes in the diff). The title is manually created by the diff
      author, so it must be good enough. When using that information from the PR
      title, ignore all prefix or suffix parts in () or []: e.g. if given
      "feat(abc): bla bla [xyz]", ignore "(abc)" and "[xyz]" parts.
    - Do not duplicate the title in the Pull Request Summary! E.g. don't say
      something like "This pull request titled "..." does ...". The title
      information is already in the PR title, no sense in duplicating it in the
      summary.
    - ALWAYS start the Pull Request Summary text with a present continuous verb,
      e.g. "Adding XYZ", "Fixing XYZ" or any other present continuous verb that
      is best suitable there.
    - NEVER MENTION COMMIT SHA HASHES! NEVER MENTION COMMIT SHA HASH PREFIXES!
      Especially in backtick-quotes. Those hashes are ephemeral, it makes no
      sense to include them in the summary.
    - NEVER TRY TO FILL ANY TEST STEPS OR TEST PLAN in the summary; only build
      human-readable text. Your goal is NOT to create test plans. Your goal is
      to only fill the human readable summary of the change.

    # Bottom part: Essential Code Lines (Optional)

    - In the end, extract most "Essential Code Lines" from the diff (NOT MORE
      THAN 5 LINES!!!) and mention top 5 of those extracted lines in a code
      block (markdown; use the language tag in triple-backtick syntax).
    - Use "Essential Code Lines" as a subheader.
    - Make sure that those "Essential Code Lines" ARE SOMEHOW RELATED to the PR
      title provided. Typically, when author of a PR writes its title, it has
      some essential code lines in mind; try to infer them based on the title if
      possible or when unsure.
    - If there is nothing interesting to extract, or if the markdown code
      triple-backtick text you're about to inject is empty, simply skip this
      ending of block 2 and don't even mention that it's absent (since it's
      optional).
    - Otherwise, append the corresponding explanation of the Essential Code
      Lines as well (in a very short form, as 1 paragraph below the code block).
    - Avoid pompous phrases like "This is a crucial step", "This change is
      foundational" etc.
    - NEVER TREAT import (or module inclusion, require etc.) statements as
      Essential Code Lines (i.e. whatever is a common boilerplate, should be
      disregarded). The idea is to not overload the description with code
      samples, but still highlight the most important part of the diff, its
      ESSENCE.
    - In the Essential Code lines, remember that your input is a diff, so most
      of the lines will have "+" or "-" prefix. You must remove that prefixes,
      so only the code snippet remains.
"""
AI_DEFAULT_PR_TEMPLATE = "## Summary\n\n## How was this tested?\n"
AI_DEFAULT_MODEL = "gpt-4o-mini"
AI_DEFAULT_MAX_TOKENS = 800
AI_DEFAULT_TEMPERATURE = 0.2

try:
    TERMINAL_SIZE = os.get_terminal_size()
    os.environ["COLUMNS"] = str(TERMINAL_SIZE.columns)
    os.environ["LINES"] = str(TERMINAL_SIZE.lines)
except:
    TERMINAL_SIZE = os.terminal_size(  # type: ignore
        (
            int(os.environ.get("COLUMNS", "512")),
            int(os.environ.get("LINES", "40")),
        )
    )


#
# A parsed commit from the local git folder.
#
@dataclass
class Commit:
    hash: str
    url: str | None
    title: str
    description: str
    branch: str | None
    decoration_branches: list[str]


#
# A parsed remote PR loaded from GitHub.
#
@dataclass
class Pr:
    number: int
    title: str
    body: str
    base_branch: str
    head_branch: str
    url: str
    review_decision: PrReviewDecision
    state: Literal["OPEN", "CLOSED", "MERGED"]
    auto_merge_status: Literal["ENABLED", "DISABLED"]
    labels: list[str]
    commit_hashes: list[str]

    def is_accidentally_merged(self) -> bool:
        return self.state == "MERGED" and self.base_branch.startswith(BRANCH_PREFIX)

    def is_closed_and_non_openable(self) -> bool:
        return self.state == "CLOSED" and len(self.commit_hashes) == 0


#
# A pair of commit hash and branch name.
#
@dataclass
class Branch:
    hash: str
    branch: str


#
# Settings stored in a JSON file.
#
@dataclass
class Settings:
    ai_api_key: str
    ai_model: str
    ai_max_tokens: int
    ai_temperature: float
    ai_generated_snippets: list[str]


#
# AI prompt information.
#
@dataclass
class AiPrompt:
    who_you_are: str
    prompt: str
    input: str


#
# An AI-injected text, which is a template with AI_PLACEHOLDER, plus a
# replacement text for that placeholder.
#
@dataclass
class AiInjectedText:
    template_with_placeholder: str
    text: str


#
# Some data passed from the main process to each individual child process call
# within "git rebase -i", for each commit in the stack.
#
@dataclass
class InRebaseInteractiveData:
    commit_index_one_based: int
    total_commits_in_stack: int

    @staticmethod
    def parse(str: str) -> InRebaseInteractiveData | None:
        if m := re.match(r"^(\d+)/(\d+)$", str):
            return InRebaseInteractiveData(
                commit_index_one_based=int(m.group(1)),
                total_commits_in_stack=int(m.group(2)),
            )
        return None

    def stringify(self) -> str:
        return f"{self.commit_index_one_based}/{self.total_commits_in_stack}"


BranchPushResult = Literal["pushed", "up-to-date"]

BranchType = Literal["base", "head"]

PrUpsertResult = Literal["created", "updated", "up-to-date", "reopened"]

PrReviewDecision = Literal["APPROVED", "REVIEW_REQUIRED", "CHANGES_REQUESTED"]

CommitUpdateResult = Literal["updated", "up-to-date"]

T = TypeVar("T")


class Main:
    settings: Settings | None = None
    in_rebase_interactive: InRebaseInteractiveData | None = None
    draft: bool = False
    debug: bool = False
    debug_force_push_branches: bool = False
    debug_skip_update_existing_prs: bool = False
    login: str
    remote: str
    remote_base_branch: str = "master"

    #
    # Main entry point.
    #
    def run(self):
        self.debug_log_argv()

        # A work-around for ClickUp Inc.
        os.environ["ALLOW_ADMIN_OVERRIDE"] = "1"

        parser = argparse.ArgumentParser(
            description="Pushes local commits as stacked PRs on GitHub and keeps them in sync.",
        )
        parser.add_argument(
            "--debug",
            default=False,
            action="store_true",
            help="print git and gh command lines",
        )
        parser.add_argument(
            "--debug-force-push-branches",
            default=False,
            action="store_true",
            help="if passed, forces all branches to be re-created and re-pushed",
        )
        parser.add_argument(
            "--draft",
            default=False,
            action="store_true",
            help="create pull requests as drafts",
        )
        args = parser.parse_args()

        self.settings_load()

        self.in_rebase_interactive = InRebaseInteractiveData.parse(
            os.environ.get(INTERNAL_IN_REBASE_INTERACTIVE_VAR, "")
        )

        self.draft = args.draft
        self.debug = args.debug
        self.debug_force_push_branches = args.debug_force_push_branches
        self.debug_skip_update_existing_prs = bool(
            os.environ.get(INTERNAL_SKIP_UPDATE_EXISTING_PRS_VAR)
        )

        if not self.in_rebase_interactive:
            self.gh_verify_version()
            self.git_verify_version()
            self.self_update()
            # For debug purposes only.
            self.shell_no_throw(["git", "status"])
            self.shell_no_throw(["git", "log", "--graph", "--decorate", "-3"])

        # Below is the real work.
        self.login = self.cache_through(
            "gh_get_current_login",
            self.gh_get_current_login,
        )
        self.remote = self.cache_through(
            "git_get_current_remote",
            self.git_get_current_remote,
        )
        self.remote_base_branch = self.cache_through(
            "git_get_current_remote_base_branch",
            self.git_get_current_remote_base_branch,
        )
        if not self.in_rebase_interactive:
            self.run_all()
        else:
            self.run_in_rebase_interactive(data=self.in_rebase_interactive)

    #
    # Assuming all PRs in the stack already have PR URLs in their description,
    # pushes the updated branches and updates the PRs descriptions with the full
    # list of commits in the stack.
    #
    def run_all(self):
        commits = self.git_get_commits()
        if len(commits) == 0:
            self.print_header(
                f"There are no local commits on top of {self.remote}. Commit something and rerun."
            )
            return

        task_upsert_labels = Task(self.gh_upsert_labels)
        tasks_gh_get_pr = dict(
            (url, Task(self.gh_get_pr, url=url))
            for url in (c.url for c in reversed(commits) if c.url)
        )
        task_upsert_labels.wait()
        prs_by_url = dict((url, task.wait()) for (url, task) in tasks_gh_get_pr.items())

        self.debug_log_commits(commits=commits, prs_by_url=prs_by_url)

        self.process_validate_commits(commits=commits, prs_by_url=prs_by_url)

        # Inject "Pull Request" URL to commit descriptions starting from the
        # commit which doesn't have it. This is a heavy-weighted process which
        # is run for each commit in an interactive rebase, starting from bottom
        # to the very top.
        commit_with_no_url: Commit | None = None
        commit_hashes_to_push_branch: list[str] = []
        for commit in reversed(commits):
            if self.debug_force_push_branches:
                commit_with_no_url = commit
                break
            elif not commit.url:
                commit_with_no_url = commit
                break
            else:
                pr = prs_by_url[commit.url]
                if pr.is_closed_and_non_openable():
                    # If the PR is closed and has 0 commits, there is no way to
                    # reopen it or update its base branch using GitHub API. So
                    # we can only re-create such a PR.
                    commit_with_no_url = commit
                    break
                if pr.is_accidentally_merged():
                    # If the PR was accidentally merged, we need to create a new
                    # PR for the commit, as if no PR existed at all.
                    commit_with_no_url = commit
                    break
                else:
                    # Push this branch and see whether GitHub says that it was
                    # up to date or not.
                    commit_hashes_to_push_branch.append(commit.hash)

        # Some commits have no related PRs (no GitHub URLs in the message)?
        # Create missing PRs and amend their messages (via rebase interactive).
        if commit_with_no_url:
            self.process_create_missing_prs_in_rebase_interactive(
                earliest_hash=commit_with_no_url.hash,
                commits=commits,
            )
            commits = self.git_get_commits()

        # Update PRs for commits; they now all have URLs. Do not push branches
        # for commits for which we have just created missing PRs above (if any),
        # i.e. only push branches for commit_hashes_to_push_branch. This allows
        # to not re-trigger GitHub Actions. Notice that these branches WILL be
        # pushed next time, because we updated the commit message (added "Pull
        # Request" header with the just-created PR URL there which is impossible
        # to predict in advance), i.e. the sequence is "push branch - create PR
        # - update commit message - push branch", but the last step of this
        # sequence is delayed till the next run, when the author has some other
        # updates to piggy-back likely.
        if not self.debug_skip_update_existing_prs:
            self.process_update_existing_prs_from_commits_with_urls(
                commits=commits,
                commit_hashes_to_push_branch=commit_hashes_to_push_branch,
            )

    #
    # Runs the sync for only the top commit. This is an internal command which
    # is used during interactive rebase.
    #
    # This workflow runs only when the tool finds a commit that has no PR URL in
    # its description yet. If all commits have existing PR URLs (including the
    # case when they were reordered), the interactive rebase workflow is not
    # run; instead, the branches are just pushed, and the PRs are updated.
    #
    def run_in_rebase_interactive(self, *, data: InRebaseInteractiveData):
        remote_commit = self.git_get_commits(
            latest_ref=f"remotes/{self.remote}/{self.remote_base_branch}",
            count_back_in_time=1,
        )[0]
        commits = self.git_get_commits(latest_ref="HEAD", count_back_in_time=2)
        if len(commits) < 2:
            raise UserException("The repository must have at least 2 commits")
        commit, prev_commit = commits[0], commits[1]

        self.print_header(f"Processing commit: {clean_title(commit.title)}")

        to_push: list[Branch] = []
        if prev_commit.hash != remote_commit.hash:
            prev_commit.branch = self.process_commit_infer_branch(commit=prev_commit)
            to_push.append(Branch(hash=prev_commit.hash, branch=prev_commit.branch))
        commit.branch = self.process_commit_infer_branch(commit=commit)
        to_push.append(Branch(hash=commit.hash, branch=commit.branch))

        push_results = self.git_push_branches(branches=to_push)

        if prev_commit.branch in push_results:
            self.print_branch_result(
                type="base",
                branch=prev_commit.branch,
                result=push_results[prev_commit.branch],
            )
        else:
            self.print_branch_result(
                type="base",
                branch=self.remote_base_branch,
                result="up-to-date",
            )

        self.print_branch_result(
            type="head",
            branch=commit.branch,
            result=push_results[commit.branch],
        )

        new_pr_title = commit.title
        new_pr_body = None
        new_pr_is_instead_of = None

        if commit.url:
            pr = self.gh_get_pr(url=commit.url)
            if pr.is_closed_and_non_openable():
                self.print_pr_closed_and_non_openable_no_commits()
                commit.url = None
                new_pr_is_instead_of = "closed and non-openable"
                new_pr_title = pr.title
                new_pr_body = pr.body
            elif pr.is_accidentally_merged():
                self.print_pr_accidentally_merged()
                commit.url = None
                new_pr_is_instead_of = "accidentally merged"
                new_pr_title = pr.title
                new_pr_body = pr.body

        if not commit.url:
            assert (
                commit.branch is not None
            ), f"commit {commit.hash} branch must be resolved"
            if new_pr_body is None:
                template_file = find_file_in_parents(PR_TEMPLATE_FILE)
                if template_file:
                    with open(template_file) as file:
                        pr_template = file.read()
                else:
                    pr_template = None
                new_pr_body = self.process_build_pr_body_with_ai(
                    commit=commit,
                    pr_template=pr_template,
                )
            url, result = self.gh_create_pr(
                base_branch=prev_commit.branch,
                head_branch=commit.branch,
                title=new_pr_title,
                body=new_pr_body,
                is_middle_pr=(
                    data.commit_index_one_based != 1
                    and data.commit_index_one_based != data.total_commits_in_stack
                ),
            )
            if result != "up-to-date":
                self.print_pr_created(
                    comment=(
                        f"instead of {new_pr_is_instead_of}"
                        if new_pr_is_instead_of
                        else None
                    ),
                    debug_url=url if self.debug_skip_update_existing_prs else None,
                )
            commit.url = url

            result = self.git_update_current_commit_message(
                header_name=PR_HEADER,
                header_value=commit.url,
                header_comment=self.remote_base_branch,
            )
            if result != "up-to-date":
                self.print_commit_message_updated()
                updated_commit = self.git_get_commits(
                    latest_ref="HEAD",
                    count_back_in_time=1,
                )[0]
                self.git_assign_branch_to_hash(
                    branch=commit.branch,
                    hash=updated_commit.hash,
                )

    #
    # Checks that the tool can process this stack of commits at all.
    #
    def process_validate_commits(
        self,
        *,
        commits: list[Commit],
        prs_by_url: dict[str, Pr],
    ):
        commits_old_to_new = list(reversed(commits))
        for i, commit in list(enumerate(commits_old_to_new))[1:]:
            pr = prs_by_url.get(commit.url, None) if commit.url else None
            if pr and pr.auto_merge_status == "ENABLED":
                prev_commit = commits_old_to_new[i - 1]
                prev_pr = (
                    prs_by_url.get(prev_commit.url, None) if prev_commit.url else None
                )
                if not prev_pr or prev_pr.head_branch != pr.base_branch:
                    raise UserException(
                        f"PR {pr.url} is auto-mergeable.\n"
                        + "I can't update it at GitHub without risking to auto-merge to a wrong branch.\n"
                        + "Please disable auto-merge for the PR and try again."
                    )

    #
    # Starting from commit with hash earliest_hash which has no PR URL in the
    # description, creates the missing PRs and updates commits descriptions with
    # the returned URL.
    #
    def process_create_missing_prs_in_rebase_interactive(
        self,
        *,
        earliest_hash: str,
        commits: list[Commit],
    ):
        try:
            commits_to_rebase = list(
                dropwhile(lambda c: c.hash != earliest_hash, reversed(commits))
            )
            assert (
                commits_to_rebase
            ), f"earliest_hash={earliest_hash} must be in commits={commits}"
            cmd = shlex.join(
                [
                    __file__,
                    *(["--debug"] if self.debug else []),
                    *(
                        ["--debug-force-push-branches"]
                        if self.debug_force_push_branches
                        else []
                    ),
                    *(["--draft"] if self.draft else []),
                ]
            )
            todo = "".join(
                [
                    f"pick {c.hash} {c.title}\n"
                    + f"exec {INTERNAL_IN_REBASE_INTERACTIVE_VAR}="
                    + shlex.quote(
                        InRebaseInteractiveData(
                            commit_index_one_based=(
                                len(commits) - len(commits_to_rebase) + i + 1
                            ),
                            total_commits_in_stack=len(commits),
                        ).stringify()
                    )
                    + f" {cmd}\n"
                    for [i, c] in enumerate(commits_to_rebase)
                ]
            )
            self.git_rebase_interactive_exec(
                earliest_hash=earliest_hash,
                skip_res=[
                    r"^You can fix the problem, and then run$",
                    r"^\s*git rebase --continue$",
                ],
                todo=f"\n{todo}",
            )
        except (CalledProcessError, KeyboardInterrupt) as e:
            self.shell_no_throw(["git", "rebase", "--abort"])
            raise e

    #
    # Iterates over the list of commits with URLs and pushes their branches to
    # update the corresponding PRs.
    #
    def process_update_existing_prs_from_commits_with_urls(
        self,
        *,
        commits: list[Commit],
        commit_hashes_to_push_branch: list[str],
    ):
        # We must iterate from the oldest commit to the newest one, because
        # previous commit PR's branch becomes the next commit PR's base branch.
        commits_old_to_new = list(reversed(commits))

        # Collect commits/branches that need to be pushed.
        to_push: dict[str, Branch] = {}
        for commit in commits_old_to_new:
            if (
                self.debug_force_push_branches
                or commit.hash in commit_hashes_to_push_branch
            ):
                commit.branch = self.process_commit_infer_branch(commit=commit)
                to_push[commit.branch] = Branch(hash=commit.hash, branch=commit.branch)

        push_results = {}
        if to_push and self.git_commits_were_not_reordered(commits=commits):
            self.print_header(
                f"Pushing {len(to_push)} PR head{'s' if len(to_push) > 1 else ''} atomically:"
            )
            push_results = self.git_push_branches(branches=[*to_push.values()])
            for branch, result in push_results.items():
                self.print_branch_result(
                    type="head",
                    branch=branch,
                    result=result,
                )

        for i, commit in enumerate(commits_old_to_new):
            self.print_header(f"Updating PR: {clean_title(commit.title)}")

            # If we did not push this branch yet (e.g. because the commits were
            # reordered), push the branches sequentially, interleaving the
            # pushes with PR updates. This minimizes the chance that GitHub will
            # close some of PRs or mark them as merged in case of commits
            # reordering (but it still may happen rarely, just unlikely).
            if commit.branch in to_push and commit.branch not in push_results:
                push_results.update(
                    self.git_push_branches(branches=[to_push[commit.branch]])
                )
                result = push_results[commit.branch]
                if result == "pushed":
                    self.print_branch_result(
                        type="head",
                        branch=str(commit.branch),
                        result=result,
                    )

            pr, result = self.process_update_pr(
                prev_commit=commits_old_to_new[i - 1] if i > 0 else None,
                commit=commit,
                commits=commits,
            )
            self.print_pr_result(
                url=pr.url,
                result=result,
                review_decision=pr.review_decision,
            )
            commits_old_to_new[i].branch = pr.head_branch

    #
    # For a commit, infers its corresponding remote branch name by either
    # querying it from the PR (when commit.url is set), or by building it from
    # the commit title and hash.
    #
    def process_commit_infer_branch(
        self,
        *,
        commit: Commit,
    ) -> str:
        if commit.url:
            pr = self.gh_get_pr(url=commit.url)  # likely a cache hit
            return pr.head_branch
        else:
            return self.build_branch_name(
                title=commit.title,
                commit_hash=commit.hash,
            )

    #
    # Updates PR fields:
    # - base branch (in case the commits were reordered, or some of them were
    #   merged into the remote, so the the beginning of the stack started
    #   pointing not to a branch, but to a remote tip)
    # - full stack list in the description
    # - git-grok related labels
    #
    # It's BTW not possible to update the head branch once PR is created.
    #
    def process_update_pr(
        self,
        *,
        prev_commit: Commit | None,
        commit: Commit,
        commits: list[Commit],
    ) -> tuple[Pr, PrUpsertResult]:
        pr_numbers: list[int] = []
        pr_number_current: int | None = None
        for c in commits:
            assert c.url is not None
            if m := re.search(r"/(\d+)$", c.url):
                pr_numbers.append(int(m.group(1)))
                if c.hash == commit.hash:
                    pr_number_current = int(m.group(1))

        assert commit.url is not None
        pr = self.gh_get_pr(url=commit.url)
        body = pr.body

        self.settings_load()
        if self.settings:
            injected_text = body_convert_to_injected_text(
                body=pr.body,
                ai_generated_snippets=self.settings.ai_generated_snippets,
            )
            if injected_text is not None:
                rules_hash = ai_hash_extract(injected_text.text)
                if rules_hash != self.ai_get_rules_hash():
                    commit.title = pr.title
                    body = self.process_build_pr_body_with_ai(
                        commit=commit,
                        pr_template=injected_text.template_with_placeholder,
                    )

        return self.gh_update_pr(
            pr=pr,
            base_branch=prev_commit.branch if prev_commit else None,
            head_commit_hash=commit.hash,
            pr_numbers=pr_numbers,
            pr_number_current=pr_number_current,
            body=body,
        )

    #
    # Builds PR body with AI (or returns the content of the PR template file).
    #
    def process_build_pr_body_with_ai(
        self,
        *,
        commit: Commit,
        pr_template: str | None,
    ) -> str:
        if not self.settings:
            return pr_template or ""

        self.print_ai_generating()
        pr_template = pr_template or AI_DEFAULT_PR_TEMPLATE

        injected_text = self.ai_generate_injected_text(
            prompt_inject_placeholder=(
                pr_template
                if AI_PLACEHOLDER in pr_template
                else self.ai_build_prompt_inject_placeholder(pr_template=pr_template)
            ),
            prompt_generate_summary=self.ai_build_prompt_generate_summary(
                title=commit.title,
                diff=self.git_get_commit_diff(commit_hash=commit.hash),
            ),
        )
        self.settings.ai_generated_snippets.insert(0, injected_text.text)
        self.settings_merge_save()

        return body_build_from_injected_text(injected_text=injected_text)

    #
    # Verifies that the gh supports e.g. autoMergeRequest field.
    #
    def gh_verify_version(self):
        out = self.shell(["gh", "--version"])
        if not (m := re.search(r"(version\s+|v)(\d+(\.\d+)+)", out)) or parse_version(
            m.group(2)
        ) < parse_version(GH_MIN_VERSION):
            raise UserException(
                f"The tool requires gh version {GH_MIN_VERSION} or higher; please upgrade. Got:\n{out.strip()}"
            )

    #
    # Returns current GitHub login.
    #
    def gh_get_current_login(self) -> str:
        return self.shell(["gh", "api", "user", "--jq", ".login"])

    #
    # Returns current git folder owner name and repository name.
    #
    def gh_get_current_repo_owner_and_name(self) -> tuple[str, str]:
        out_str = self.shell(
            [
                "gh",
                "repo",
                "view",
                "--json",
                "owner,name",
                "--jq",
                "[.owner.login,.name]",
            ]
        )
        return tuple(json.loads(out_str))

    #
    # Pre-creates git-grok related labels.
    #
    def gh_upsert_labels(self):
        try:
            self.shell(
                [
                    "gh",
                    "label",
                    "create",
                    MIDDLE_PR_LABEL,
                    "-d",
                    MIDDLE_PR_LABEL_DESCRIPTION,
                    "-c",
                    MIDDLE_PR_LABEL_COLOR,
                ],
            )
            # It's faster to ignore the "already exists" error than to use
            # --force flag.
        except CalledProcessError as e:
            if "already exists" not in e.stderr:
                raise

    #
    # Creates a GitHub PR between two existing branches. The description of the
    # new PR will NOT include the body suffix with the list of all PRs in the
    # stack, since we don't know all URLs in this list yet.
    #
    def gh_create_pr(
        self,
        *,
        base_branch: str | None,
        head_branch: str,
        title: str,
        body: str,
        is_middle_pr: bool,
    ) -> tuple[str, PrUpsertResult]:
        if not base_branch:
            base_branch = self.remote_base_branch

        body_file = TMP_BODY_FILE
        with open(body_file, "w+") as file:
            file.write(body)

        cmd = [
            "gh",
            "pr",
            "create",
            *(["--draft"] if self.draft else []),
            "--base",
            base_branch,
            "--head",
            head_branch,
            *(["--label", MIDDLE_PR_LABEL] if is_middle_pr else []),
            "--title",
            title,
            "--body-file",
            body_file,
        ]

        attempt = 0
        while True:
            returncode, output, stderr = self.shell_no_throw(cmd)
            if returncode == 0:
                return output, "created"

            if m := re.match(r".* already exists[^\n]\n(\S+)", stderr, flags=re.S):
                return m.group(1), "up-to-date"

            if attempt < 3 and (
                m := re.match(r".* was submitted too quickly", stderr, flags=re.S)
            ):
                dt = 3 + random.random() * 3
                self.debug_log_text(text=f"Waiting for {dt} seconds and retrying...")
                sleep(dt)
                attempt += 1
                continue

            raise CalledProcessError(
                returncode=returncode,
                cmd=cmd,
                output=output,
                stderr=stderr,
            )

    #
    # Updates base_branch and description suffix of the PR.
    #
    def gh_update_pr(
        self,
        *,
        pr: Pr,
        base_branch: str | None,
        head_commit_hash: str,
        pr_numbers: list[int],
        pr_number_current: int | None,
        body: str,
    ) -> tuple[Pr, PrUpsertResult]:
        if base_branch is None:
            base_branch = self.remote_base_branch

        body = body_suffix_upsert(
            body=body,
            pr_numbers=pr_numbers,
            pr_number_current=pr_number_current,
        )
        is_middle_pr = len(pr_numbers) > 0 and pr.number not in (
            pr_numbers[0],
            pr_numbers[-1],
        )

        if (
            pr.base_branch == base_branch
            and pr.body == body
            and pr.state == "OPEN"
            and (MIDDLE_PR_LABEL in pr.labels) == is_middle_pr
        ):
            return pr, "up-to-date"

        self.cache_clean(pr.url)

        upsert_result = "updated"

        if pr.state != "OPEN":
            upsert_result = "reopened"
            returncode, output, stderr = self.shell_no_throw(
                cmd := [
                    "gh",
                    "pr",
                    "reopen",
                    pr.url,
                    "--comment",
                    "Reopened by git-grok.",
                ],
            )
            if (
                returncode != 0
                and len(pr.commit_hashes) > 0
                and "Could not open the pull request" in stderr
            ):
                # https://github.com/isaacs/github/issues/361 - "The XXX branch
                # was force pushed or recreated". "We're blocking the pull
                # request reopen if the current head isn't a descendant of the
                # stored head sha (which is what the head was when the pull
                # request was closed). We are not allowing the reopen in that
                # case, because there is no good way to tell what changes have
                # happened while a pull request was closed and the head branch
                # has changed."
                self.print_pr_closed_and_has_bad_head_branch()
                temp_commit_hash = pr.commit_hashes[-1]
                self.git_push_branches(
                    branches=[Branch(hash=temp_commit_hash, branch=pr.head_branch)]
                )
                sleep(5)  # GitHub replication lag?
                self.shell(
                    [
                        "gh",
                        "pr",
                        "reopen",
                        pr.url,
                        "--comment",
                        f"Hack: temporarily force-pushing the old commit {temp_commit_hash} to the head branch {pr.head_branch} and reopening by git-grok. "
                        + "See https://github.com/isaacs/github/issues/361 for details.",
                    ],
                )
                self.git_push_branches(
                    branches=[Branch(hash=head_commit_hash, branch=pr.head_branch)]
                )
            elif returncode != 0:
                raise CalledProcessError(
                    returncode=returncode,
                    cmd=cmd,
                    output=output,
                    stderr=stderr,
                )
        self.shell(
            [
                "gh",
                "pr",
                "edit",
                pr.url,
                "--base",
                base_branch,
                *(
                    ["--add-label", MIDDLE_PR_LABEL]
                    if is_middle_pr and MIDDLE_PR_LABEL not in pr.labels
                    else (
                        ["--remove-label", MIDDLE_PR_LABEL]
                        if not is_middle_pr and MIDDLE_PR_LABEL in pr.labels
                        else []
                    )
                ),
                "--body-file",
                "-",
            ],
            input=body,
        )

        pr.base_branch = base_branch
        pr.body = body
        return pr, upsert_result

    #
    # Reads PR info from GitHub.
    #
    # We read PRs one by one and not in GraphQL-bulk for 3 reasons:
    # - Granular caching. We use cache_through() for each PR, so in the
    #   consequent interactive rebase call, it will be reused (and also, it will
    #   be reused in other places where we need to read a PR info).
    # - It is not much worse in performance, because we run the initial
    #   gh_get_pr() in parallel, in Task threads.
    # - Simplicity reason: we want to avoid GraphQL code duplication with the
    #   logic of gh_get_pr().
    #
    def gh_get_pr(self, *, url: str) -> Pr:
        out_str = self.cache_through(
            url,
            lambda: self.shell(
                [
                    "gh",
                    "pr",
                    "view",
                    url,
                    "--json",
                    "number,title,body,baseRefName,headRefName,url,reviewDecision,state,labels,autoMergeRequest,commits",
                ],
            ),
        )
        value = json.loads(out_str)
        return Pr(
            number=int(value["number"]),
            title=value["title"],
            body=value["body"],
            base_branch=re.sub(r"^refs/heads/", "", value["baseRefName"]),
            head_branch=re.sub(r"^refs/heads/", "", value["headRefName"]),
            url=value["url"],
            review_decision=value["reviewDecision"],
            state=value["state"],
            auto_merge_status="ENABLED" if value["autoMergeRequest"] else "DISABLED",
            labels=[label["name"] for label in value["labels"]],
            commit_hashes=[c["oid"] for c in value["commits"]],
        )

    #
    # Verifies Git version.
    #
    def git_verify_version(self):
        self.shell_no_throw(["git", "--version"])

    #
    # Returns current git folder remote (most often "origin").
    #
    def git_get_current_remote(self) -> str:
        [_, symbolic_ref, _] = self.shell_no_throw(
            ["git", "symbolic-ref", "-q", "HEAD"]
        )
        if not symbolic_ref:
            raise UserException(
                'To run git-grok, you must be on a branch. Check your "git status".'
            )
        push_short = self.shell(
            ["git", "for-each-ref", "--format=%(push:short)", symbolic_ref]
        )
        if not (m := re.match(r"^([^/]+)/.+$", push_short)):
            raise UserException(
                f'fatal: No configured push destination for symbolic ref "{symbolic_ref}".\n'
                + "\n"
                + "Git infers the current remote from the branch name using one of the following ways:\n"
                + '1. It checks whether "git config branch.<branch>.remote" is set explicitly.\n'
                + '2. Or, if "git config push.default" is "current" or "matching", there must be\n'
                + "   only one remote configured.\n"
            )
        return m.group(1)

    #
    # Returns the current remote branch name on GitHub.
    #
    def git_get_current_remote_base_branch(self) -> str:
        branch = self.shell(["git", "branch", "--show-current"])
        if not branch:
            path = ".git/rebase-merge/head-name"
            if file := find_file_in_parents(path):
                branch = open(file).readline().strip()
                if not (m := re.match(r"refs/heads/(.+)", branch)):
                    raise UserException(f"File {path} doesn't contain refs/heads/*")
                branch = m.group(1)
            else:
                raise UserException(
                    f"You must be on a branch or in an interactive rebase mode."
                )
        return branch

    #
    # Returns parsed commits between two refs in reverse-chronological order
    # (newest commits first, oldest last, as they're shown in "git log").
    #
    # The returned commits will have branch = None, since it's unknown yet.
    #
    def git_get_commits(
        self,
        *,
        latest_ref: str | None = None,
        earliest_ref: str | None = None,
        count_back_in_time: int | None = None,
    ) -> list[Commit]:
        if latest_ref is None:
            latest_ref = "HEAD"
        if earliest_ref is None:
            earliest_ref = f"remotes/{self.remote}/{self.remote_base_branch}"
        if count_back_in_time:
            out = self.shell(
                ["git", "log", latest_ref, f"-{count_back_in_time}", "--decorate=short"]
            )
        else:
            out = self.shell(
                ["git", "log", f"{earliest_ref}..{latest_ref}", "--decorate=short"]
            )
        commits: list[Commit] = []
        for commit in re.split(r"^(?=commit )", out, flags=re.M)[1:]:
            try:
                if not (
                    m := re.match(
                        r"commit (\w+)\s*(?:[(]([^)]*)[)])?[^\n]*\n(.*?)\n\n(.*)$",
                        commit,
                        flags=re.S,
                    )
                ):
                    raise UserException("Can't parse commit-headers-body.")

                hash, decorations, _headers, body = (
                    m.group(1),
                    (m.group(2) or "").strip(),
                    m.group(3),
                    m.group(4).rstrip(),
                )

                decoration_branches = [
                    re.sub(r"^.*->", "", v).strip()
                    for v in decorations.split(",")
                    if v.strip()
                ]

                if not (m := re.match(r"([^\n]+)\n?(.*)$", body, flags=re.S)):
                    raise UserException(f"Can't extract commit title and description.")
                title, description = m.group(1).strip(), m.group(2)

                m = re.search(PR_HEADER_RE, description, flags=re.M)
                url = m.group(1).strip() if m else None
                expected_base_branch = m.group(2).strip() if m and m.group(2) else None

                # People often times cherry-pick commits from one branch to
                # another to e.g. ship hot-fixes in production or staging
                # branch. In this case, we must ignore the PR URL, because along
                # with this cherry-pick, the commit message (with existing PR
                # URL) is copied, so we don't want to have a collision.
                if (
                    expected_base_branch is not None
                    and expected_base_branch != self.remote_base_branch
                ):
                    url = None
            except UserException as e:
                raise UserException(f"{str(e)} Commit:\n<<<\n{commit.strip()}\n>>")
            except BaseException as e:
                tb = traceback.format_exc()
                raise UserException(f"{tb}\nCommit:\n<<<\n{commit.strip()}\n>>\n\n")

            commits.append(
                Commit(
                    hash=hash,
                    url=url,
                    title=title,
                    description=description,
                    branch=None,
                    decoration_branches=decoration_branches,
                )
            )
        return commits

    #
    # Returns the diff of the provided commit.
    #
    def git_get_commit_diff(self, *, commit_hash: str) -> str:
        return self.shell(["git", "show", f"-U{AI_DIFF_CONTEXT_LINES}", commit_hash])

    #
    # Pushes multiple branches atomically to remote GitHub. Returns a dict
    # mapping branch names to their push results.
    #
    def git_push_branches(
        self,
        *,
        branches: list[Branch],
    ) -> dict[str, BranchPushResult]:
        if not branches:
            return {}
        # Git push is a quick no-op on GitHub end if the branch isn't changed
        # (it prints "Everything up-to-date"), so we always push and then verify
        # the output for the status (instead of fetching from the remote and
        # comparing the branch'es tip commit hash).
        out = self.shell(
            [
                "git",
                "push",
                "-f",
                self.remote,
                *[f"{branch.hash}:refs/heads/{branch.branch}" for branch in branches],
            ],
            stderr_to_stdout=True,
        )
        # If the hash is NOT mentioned in the output, it's either a short
        # "Everything up-to-date" message (which means that ALL branches are
        # unchanged), or THIS particular branch is up-to-date. I.e. if a branch
        # is changed, git always prints its hash in the output, on one of the
        # following formats:
        # 1. * [new branch] 10dc4f6 -> grok/...
        # 2. + 10dc4f6...b28d03e 10dc4f6 -> grok/... (forced update)
        results: dict[str, BranchPushResult] = {
            branch.branch: "up-to-date" for branch in branches
        }
        for branch in branches:
            for line in out.splitlines():
                if branch.hash in line:
                    results[branch.branch] = "pushed"
                    break
        return results

    #
    # Assigns the provided commit hash to the remote branch without pushing it.
    # This is useful after e.g. changing the commit message.
    #
    def git_assign_branch_to_hash(self, *, branch: str, hash: str):
        self.shell(
            [
                "git",
                "update-ref",
                f"refs/remotes/{self.remote}/{branch}",
                f"{hash}",
            ],
        )

    #
    # Runs an interactive rebase with the provided shell command.
    #
    def git_rebase_interactive_exec(
        self,
        *,
        earliest_hash: str,
        skip_res: list[str] = [],
        todo: str,
    ):
        self.shell_passthrough(
            [
                "git",
                "rebase",
                "--quiet",
                "--interactive",
                f"{earliest_hash}^",
            ],
            env={"GIT_SEQUENCE_EDITOR": f"echo {shlex.quote(todo)} >"},
            skip_res=[r"Executing: ", *skip_res],
        )

    #
    # Updates the message suffix (pointing to a PR URL) of the current commit
    # which is at the moment selected (activated) with interactive rebase.
    #
    def git_update_current_commit_message(
        self,
        *,
        header_name: str,
        header_value: str,
        header_comment: str | None = None,
    ) -> CommitUpdateResult:
        new_header = f"{header_name}: {header_value}"
        if header_comment:
            new_header += f" ({header_comment})"
        message = self.shell(
            ["git", "show", "--pretty=format:%B", "--no-patch"],
            no_rstrip=True,
        )
        if m := re.search(rf"^{header_name}: [^\n]+", message, flags=re.M):
            new_message = message.replace(m[0], new_header)
        else:
            new_message = f"{message.rstrip()}\n\n{new_header}"
        if new_message != message:
            self.shell(
                ["git", "commit", "--amend", "--no-verify", "-F", "-"],
                input=new_message,
            )
            return "updated"
        else:
            return "up-to-date"

    #
    # Checks whether the order of the commits on top of the upstream remained
    # the same since the last run. If so, we are e.g. allowed to push multiple
    # branches in bulk, atomically.
    #
    def git_commits_were_not_reordered(self, *, commits: list[Commit]) -> bool:
        local_commit_branches = [
            commit.branch or self.process_commit_infer_branch(commit=commit)
            for commit in commits
        ]

        remote_commit_branch_sets = [
            set(
                branch[len(self.remote + "/") :]
                for branch in commit.decoration_branches
                if branch.startswith(self.remote + "/")
            )
            for commit in self.git_get_commits(
                latest_ref=f"remotes/{self.remote}/{local_commit_branches[0]}",
            )
        ]

        self.debug_log_text(
            text=[
                f"Local commits branches order:  {str([[b] for b in local_commit_branches])}",
                f"Remote commits branch order: {str(remote_commit_branch_sets)}",
            ]
        )

        while local_commit_branches and remote_commit_branch_sets:
            local_commit_branch = local_commit_branches.pop(0)
            remote_commit_branch_set = remote_commit_branch_sets.pop(0)
            if local_commit_branch not in remote_commit_branch_set:
                return False

        if local_commit_branches or remote_commit_branch_sets:
            return False

        return True

    #
    # Returns a hash based on the static part of the prompt ("rules"). Injecting
    # this hash as a part of the AI-generated snippet allows to improve the AI
    # prompt text when developing git-grok, as a part of dog-food feedback loop.
    # E.g. if we modify AI_PROMPT and rerun git-grok for a stack, then the tool
    # will update all the existing PRs with a newly generated text (and if no
    # changes in AI_PROMPT happened, or there is no AI-generated snippet found
    # in the PR at all, then the PR body will be left untouched).
    #
    def ai_get_rules_hash(self) -> str:
        return short_hash(
            "\n".join(
                [
                    self.settings.ai_model if self.settings else "",
                    str(self.settings.ai_max_tokens if self.settings else ""),
                    str(self.settings.ai_temperature if self.settings else ""),
                    str(AI_DIFF_CONTEXT_LINES),
                    unindent(AI_WHO_YOU_ARE),
                    unindent(AI_PROMPT_INJECT_PLACEHOLDER).rstrip(),
                    unindent(AI_PROMPT_GENERATE_SUMMARY).rstrip(),
                ]
            )
        )

    #
    # Builds a prompt for the AI to inject a placeholder to the PR template.
    #
    def ai_build_prompt_inject_placeholder(
        self,
        *,
        pr_template: str,
    ) -> AiPrompt:
        pr_template = pr_template.strip() + "\n" if pr_template.strip() else ""
        excluding = f"(excluding the {AI_SEPARATOR[0:4]}... separator)"
        return AiPrompt(
            who_you_are=unindent(AI_WHO_YOU_ARE).rstrip(),
            prompt=unindent(AI_PROMPT_INJECT_PLACEHOLDER).rstrip(),
            input=f"Here is the {PR_TEMPLATE_FILE} developers use {excluding}:\n{AI_SEPARATOR}\n{pr_template}{AI_SEPARATOR}\n\n",
        )

    #
    # Builds a prompt for the AI to generate a PR description.
    #
    def ai_build_prompt_generate_summary(
        self,
        *,
        title: str,
        diff: str,
    ) -> AiPrompt:
        title = title.strip() + "\n" if title.strip() else ""
        diff = diff.strip() + "\n" if diff.strip() else ""
        excluding = f"(excluding the {AI_SEPARATOR[0:4]}... separator)"
        return AiPrompt(
            who_you_are=unindent(AI_WHO_YOU_ARE).rstrip(),
            prompt=unindent(AI_PROMPT_GENERATE_SUMMARY).rstrip(),
            input=(
                f"Here is the PR title {excluding}:\n{AI_SEPARATOR}\n{title}{AI_SEPARATOR}\n\n"
                + f"Here is the git diff {excluding}:\n{AI_SEPARATOR}\n{diff}{AI_SEPARATOR}\n\n"
            ),
        )

    #
    # Sends an AI request and returns the text response.
    #
    def ai_send_request(
        self,
        *,
        prompt: AiPrompt,
    ) -> str:
        assert self.settings
        data = json.dumps(
            {
                "model": self.settings.ai_model,
                "messages": [
                    {"role": "system", "content": prompt.who_you_are},
                    {"role": "system", "content": prompt.prompt},
                    {"role": "user", "content": prompt.input},
                ],
                "temperature": self.settings.ai_temperature,
                "max_tokens": self.settings.ai_max_tokens,
            },
            indent=2,
        )
        req = Request(
            "https://api.openai.com/v1/chat/completions",
            data=data.encode(),
            headers={
                "Authorization": f"Bearer {self.settings.ai_api_key}",
                "Content-Type": "application/json",
            },
        )
        time_start = time()
        res = None
        returncode = None
        try:
            # MacOS Python 3.10 doesn't seem to have OpenAI certs. It raises the
            # error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed:
            # unable to get local issuer certificate.
            context = ssl.create_default_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            with urlopen(req, context=context) as res:
                res = json.load(res)
                return str(res["choices"][0]["message"]["content"].strip())
        except HTTPError as e:
            res = e.read().decode()
            returncode = e.code
            raise UserException(f"{e.code} {e.reason}: {res}")
        except URLError as e:
            returncode = -1
            raise UserException(f"Network error: {e}")
        finally:
            self.debug_log_shell_command(
                cmd=["curl", "-X", "POST", req.full_url],
                input=data.strip(),
                env=None,
                out=str(res),
                took_s=time() - time_start,
                returncode=returncode,
            )

    #
    # Generates an injected text block (like PR summary).
    #
    def ai_generate_injected_text(
        self,
        *,
        prompt_inject_placeholder: AiPrompt | str,
        prompt_generate_summary: AiPrompt,
    ) -> AiInjectedText:
        task_inject_placeholder = (
            Task(lambda: prompt_inject_placeholder)
            if isinstance(prompt_inject_placeholder, str)
            else Task(self.ai_send_request, prompt=prompt_inject_placeholder)
        )
        task_generate_summary = Task(
            self.ai_send_request,
            prompt=prompt_generate_summary,
        )
        return AiInjectedText(
            template_with_placeholder=task_inject_placeholder.wait().strip(),
            text=(
                ai_hash_build(self.ai_get_rules_hash())
                + "\n"
                + task_generate_summary.wait().strip()
            ),
        )

    #
    # Runs a shell command returning results.
    #
    def shell(
        self,
        cmd: list[str],
        *,
        no_rstrip: bool = False,
        input: str | None = None,
        stderr_to_stdout: bool = False,
        env: dict[str, str] | None = None,
        comment: str | None = None,
    ) -> str:
        out = None
        returncode = 0
        time_start = time()
        try:
            out = check_output(
                cmd,
                text=True,
                stderr=subprocess.STDOUT if stderr_to_stdout else subprocess.PIPE,
                input=input,
                env={**os.environ, **env} if env else None,
            )
            return out if no_rstrip else out.rstrip()
        except CalledProcessError as e:
            returncode = e.returncode
            out = "\n".join(s for s in [e.stdout, e.stderr] if s)
            raise
        finally:
            self.debug_log_shell_command(
                cmd=cmd,
                env=env,
                out=out,
                took_s=time() - time_start,
                returncode=returncode,
                comment=comment,
            )

    #
    # Same as shell(), but never throws and returns an exit code along with the
    # outputs instead.
    #
    def shell_no_throw(
        self,
        cmd: list[str],
        *,
        no_rstrip: bool = False,
        env: dict[str, str] | None = None,
        comment: str | None = None,
    ) -> tuple[int, str, str]:
        stdout = ""
        stderr = ""
        returncode = None
        time_start = time()
        try:
            proc = Popen(
                cmd,
                text=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env={**os.environ, **env} if env else None,
            )
            stdout, stderr = proc.communicate()
            if not no_rstrip:
                stdout = stdout.rstrip()
                stderr = stderr.rstrip()
            returncode = proc.returncode
            return (
                proc.returncode,
                stdout,
                stderr,
            )
        finally:
            self.debug_log_shell_command(
                cmd=cmd,
                env=env,
                out="\n".join(s for s in [stdout, stderr] if s),
                took_s=time() - time_start,
                returncode=returncode,
                comment=comment,
            )

    #
    # Runs a shell command, but prints its output as is.
    #
    def shell_passthrough(
        self,
        cmd: list[str],
        *,
        env: dict[str, str] | None = None,
        skip_res: list[str] = [],
        comment: str | None = None,
    ):
        out = ""
        self.debug_log_shell_command(
            cmd=cmd,
            env=env,
            out="",
            took_s=0,
            returncode=None,
            comment=comment,
        )
        with Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,  # to apply skip_res to stderr as well
            text=True,
            bufsize=1,
            env={**os.environ, **env} if env else None,
        ) as pipe:
            assert pipe.stdout is not None
            for line in pipe.stdout:
                line = line.rstrip()
                if not line:
                    continue
                out += line + "\n"
                if not any(re.search(r, line, flags=re.S) for r in skip_res):
                    print(line)
                    sys.stdout.flush()
            returncode = pipe.wait()
            if returncode != 0:
                raise CalledProcessError(returncode=returncode, cmd=cmd)

    #
    # Loads settings from a JSON file.
    #
    def settings_load(self):
        self.settings = None
        if not (path := find_file_in_parents(SETTINGS_FILE)):
            return
        with open(path, "r") as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_SH)
            try:
                data = json.loads(f.read())
                if not isinstance(data, dict):
                    raise UserException(f"Settings must be a JSON object")

                data = cast(dict[str, Any], data)
                ai_api_key = data.get("ai_api_key", None)
                ai_model = data.get("ai_model", AI_DEFAULT_MODEL)
                ai_max_tokens = data.get("ai_max_tokens", AI_DEFAULT_MAX_TOKENS)
                ai_temperature = data.get("ai_temperature", AI_DEFAULT_TEMPERATURE)
                ai_generated_snippets = data.get("ai_generated_snippets", [])

                if not isinstance(ai_api_key, str):
                    raise UserException(f"No string key ai_api_key")
                if not isinstance(ai_model, str):
                    raise UserException(f"Invalid type for ai_model (must be a string)")
                if not isinstance(ai_max_tokens, int):
                    raise UserException(
                        f"Invalid type for ai_max_tokens (must be an int)"
                    )
                if not isinstance(ai_temperature, float) and not isinstance(
                    ai_temperature, int
                ):
                    raise UserException(
                        f"Invalid type for ai_temperature (must be a float or int)"
                    )
                if not isinstance(ai_generated_snippets, list) or not all(
                    isinstance(s, str) for s in cast(list[Any], ai_generated_snippets)
                ):
                    raise UserException(
                        f"Invalid type for ai_generated_snippets (must be a list of strings)"
                    )
            except UserException as e:
                raise UserException(f"{e} in settings file {path}")
            self.settings = Settings(
                ai_api_key=ai_api_key,
                ai_model=ai_model,
                ai_max_tokens=ai_max_tokens,
                ai_temperature=float(ai_temperature),
                ai_generated_snippets=cast(list[str], ai_generated_snippets or []),
            )

    #
    # Saves the dynamic part of the settings to a JSON file.
    #
    def settings_merge_save(self):
        if not self.settings:
            return
        search_dir = os.path.dirname(SETTINGS_FILE)
        if not (dir := find_file_in_parents(search_dir)):
            raise UserException(
                f"Failed to find {search_dir} in parents to store settings"
            )
        path = f"{dir}/{os.path.basename(SETTINGS_FILE)}"
        with open(path, "a+") as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            f.seek(0)
            data = json.loads(f.read())
            if not isinstance(data, dict):
                raise UserException(f"Settings must be a JSON object: {path}")
            data = cast(dict[str, Any], data)
            data["ai_generated_snippets"] = [
                *self.settings.ai_generated_snippets,
                *(
                    set(data.get("ai_generated_snippets", []))
                    - set(self.settings.ai_generated_snippets)
                ),
            ][:AI_MAX_REMEMBERED_SNIPPETS]
            f.seek(0)
            f.truncate()
            f.write(f"{json.dumps(data, indent=2)}\n")

    #
    # Caches the return value of a function in environment and returns it next time
    # it's requested.
    #
    def cache_through(self, key: str, func: Callable[[], str]) -> str:
        key = hashify(key)
        var = f"{INTERNAL_ENV_VAR_PREFIX}_{key}"
        value = os.environ.get(var)
        if value is None:
            value = func()
            os.environ[var] = value
        else:
            self.debug_log_shell_command(
                cmd=["printenv", var],
                env=None,
                out=value,
                took_s=0,
                returncode=None,
                comment="cache hit",
            )
        return value

    #
    # Removes the cached value for the key.
    #
    def cache_clean(self, key: str):
        key = hashify(key)
        var = f"{INTERNAL_ENV_VAR_PREFIX}_{key}"
        os.environ.pop(var, None)

    #
    # Prints a status after a branch push attempt happened.
    #
    def print_branch_result(
        self,
        *,
        type: BranchType,
        branch: str,
        result: BranchPushResult,
    ):
        max_len = max(
            MIN_BRANCH_LEN_IN_PRINT,
            TERMINAL_SIZE.columns - (11 + 4 + len(self.remote) + 10) - 5,
        )
        if len(branch) > max_len:
            branch = branch[0:max_len] + ""
        print(f"   {type}: {self.remote}/{branch} ({result})")
        sys.stdout.flush()

    #
    # Prints a "Generating with AI" message.
    #
    def print_ai_generating(self):
        print("   generating PR description with AI (patience!)")
        sys.stdout.flush()

    #
    # Prints a "PR created" message.
    #
    def print_pr_created(
        self,
        *,
        comment: str | None = None,
        debug_url: str | None = None,
    ):
        # ATTENTION: we DO NOT print the newly created PR URL here (except for
        # debugging or tests purposes), because it lacks the footer with links
        # to all other PRs in the stack yet. If someone starts editing it before
        # that footer is added (later, during the next stage), then they'll risk
        # to lose the text! So we have no choice but to force the user to wait.
        # The URL will anyway be printed at the "Updating PR" stage, we don't
        # need to double-print it.
        print(
            f"   new Pull Request created"
            + (f" ({comment})" if comment else "")
            + (f": {debug_url}" if debug_url else "")
        )
        sys.stdout.flush()

    #
    # Prints a "PR has bad branch" message.
    #
    def print_pr_closed_and_has_bad_head_branch(self):
        print(f"   this closed PR has a bad head branch, so trying a hack")
        sys.stdout.flush()

    #
    # Prints a "PR cannot be opened" message.
    #
    def print_pr_closed_and_non_openable_no_commits(self):
        print(
            f"   this closed PR has 0 commits and cannot be reopened, so recreating"
        )
        sys.stdout.flush()

    #
    # Prints a "PR was accidentally merged" message.
    #
    def print_pr_accidentally_merged(self):
        print(f"   this PR was accidentally merged into {BRANCH_PREFIX}*, recreating")
        sys.stdout.flush()

    #
    # Prints a status after a commit message was updated locally.
    #
    def print_commit_message_updated(self):
        print(f"   added PR URL to commit's message to bind them")
        sys.stdout.flush()

    #
    # Prints a status after a PR was created/updated on GitHub.
    #
    def print_pr_result(
        self,
        *,
        url: str,
        result: PrUpsertResult,
        review_decision: PrReviewDecision,
    ):
        icon = (
            review_decision == "APPROVED"
            and ""
            or review_decision == "CHANGES_REQUESTED"
            and ""
            or ""
        )
        print(f"  {icon} {url} ({result})")
        sys.stdout.flush()

    #
    # Prints the heading message of an operation.
    #
    def print_header(self, header: str):
        print(header)
        sys.stdout.flush()

    #
    # Logs the current command line arguments.
    #
    def debug_log_argv(self):
        text = shlex.join(sys.argv).strip()
        for var in [
            INTERNAL_IN_REBASE_INTERACTIVE_VAR,
            INTERNAL_SKIP_UPDATE_EXISTING_PRS_VAR,
        ]:
            value = os.environ.get(var)
            if value:
                text = f"{var}={shlex.quote(value)} {text}"
        self.debug_log_text(text=text)

    #
    # Logs a list of commits and their corresponding PR statuses.
    #
    def debug_log_commits(
        self,
        *,
        commits: list[Commit],
        prs_by_url: dict[str, Pr],
    ):
        max_url_len = max(
            [len(c.url) for c in commits if c.url],
            default=0,
        )
        max_state_len = max(
            [len(pr.state) for pr in prs_by_url.values()],
            default=0,
        )
        max_head_branch_len = max(
            [len(pr.head_branch) for pr in prs_by_url.values()],
            default=0,
        )
        lines: list[str] = []
        for c in commits:
            url = c.url or "-"
            pr = prs_by_url.get(url)
            state = pr.state if pr else "-"
            head_branch = pr.head_branch if pr else "-"
            lines.append(
                f"{c.hash} | {url.ljust(max_url_len)} | {head_branch.ljust(max_head_branch_len)} | {state.ljust(max_state_len)} | {c.title}"
            )
        self.debug_log_text(text=lines)

    #
    # Logs a command which is run by the tool.
    #
    def debug_log_shell_command(
        self,
        *,
        cmd: list[str],
        env: dict[str, str] | None,
        input: str | None = None,
        out: str | None,
        took_s: float,
        returncode: int | None,
        comment: str | None = None,
    ):
        rows: list[tuple[tuple[int, int, int], str]] = []

        rows.append(
            (
                DEBUG_COLOR_GRAY_1,
                "$ "
                + (
                    " ".join([f"{k}={shlex.quote(v)}" for (k, v) in env.items()]) + " "
                    if env
                    else ""
                )
                + shlex.join(cmd).strip()
                + (f" # took {int(took_s * 1000)} ms" if took_s else "")
                + (f" # returncode={returncode}" if returncode else "")
                + (
                    f" # ran in a parallel thread"
                    if current_thread() is not main_thread()
                    else ""
                )
                + (f" # {comment}" if comment else ""),
            )
        )

        if input:
            rows.append((DEBUG_COLOR_GRAY_1, input))

        if out:
            rows.append(
                (
                    DEBUG_COLOR_GRAY_2,
                    ("Output:\n" if input else "")
                    + re.sub(
                        r"^",
                        "  ",
                        re.sub(r"\n([| \t]*\n)+", "\n", out.rstrip()),
                        flags=re.M,
                    ),
                )
            )

        if os.environ.get(INTERNAL_IN_REBASE_INTERACTIVE_VAR):
            rows = [(row[0], re.sub(r"^", "> ", row[1], flags=re.M)) for row in rows]

        self.debug_log_text(text=[row[1] for row in rows])
        if self.debug:
            for row in rows:
                print(colored(*row[0], row[1]))
            sys.stdout.flush()

    #
    # Logs a text block (prepended by date) to the debug file.
    #
    def debug_log_text(self, *, text: str | list[str]):
        try:
            with open(DEBUG_FILE, "a+") as file:
                file.write(
                    f"=== {datetime.now()}"
                    + (
                        f" (in rebase interactive, {self.in_rebase_interactive.commit_index_one_based}/{self.in_rebase_interactive.total_commits_in_stack})"
                        if self.in_rebase_interactive
                        else ""
                    )
                    + "\n"
                )
                if not isinstance(text, str):
                    text = "\n".join(text)
                file.write(f"{text.rstrip()}\n")
                file.write("\n")
        except:
            pass

    #
    # Tries to self-update the tool by pulling from git.
    #
    def self_update(self):
        dir = os.path.dirname(os.path.realpath(__file__))
        if not os.path.exists(f"{dir}/.git"):
            return
        self.shell_no_throw(
            ["sh", "-c", f"cd {shlex.quote(dir)} && git pull --rebase"],
            comment="self-update",
        )

    #
    # Builds branch name from commit title.
    #
    def build_branch_name(self, *, title: str, commit_hash: str) -> str:
        branch = title
        branch = branch.strip()
        branch = re.sub(r"\[[-\w]+\]$", "", branch)
        branch = branch.lower()
        branch = re.sub(r"^\W+|\W+$", "", branch)
        branch = re.sub(r"\W+", "-", branch)
        branch += f"-to-{self.remote_base_branch}-{commit_hash[0:4]}"
        return f"{BRANCH_PREFIX}{self.login}/{branch}"


#
# Splits a list into chunks of size n.
#
def chunk(lst: list[T], n: int) -> list[list[T]]:
    res: list[list[T]] = []
    for i in range(0, len(lst), n):
        res.append(lst[i : i + n])
    return res


#
# Returns a colored text.
#
def colored(r: int, g: int, b: int, text: str) -> str:
    return f"\033[38;2;{r};{g};{b}m{text}\033[0m"


#
# Traverse the filesystem from the current directory to root trying to find a
# matching relative file path in parent directories.
#
def find_file_in_parents(file: str) -> str | None:
    path = os.getcwd()
    old_path = ""
    while old_path != path:
        full = os.path.join(path, file)
        if os.path.exists(full):
            return full
        old_path = path
        path = os.path.dirname(path)
    return None


#
# Cleans a title from useless prefix/suffix.
#
def clean_title(title: str) -> str:
    title = title.strip()
    title = re.sub(r"^\w+[()\[\]\w]*:\s+", "", title)
    title = re.sub(r"\[[-\w]+\]$", "", title)
    title = title.strip()
    max_len = max(
        MIN_TITLE_LEN_IN_PRINT,
        TERMINAL_SIZE.columns - 22,
    )
    if len(title) > max_len:
        title = title[0:max_len] + ""
    return title


#
# Removes common leading whitespace from each line.
#
def unindent(text: str) -> str:
    text = re.sub(r"^([ \t\r]*\n)+", "", text, flags=re.S).rstrip()
    matches = re.match(r"^[ \t]+", text, flags=re.S)
    if matches:
        text = re.sub(f"^{matches.group(0)}", "", text, flags=re.M)
    return text


#
# If there is no body suffix in the body, appends it. Otherwise, replaces the
# previous occurrence of the body suffix (heuristically found) with the new one.
#
def body_suffix_upsert(
    *,
    body: str,
    pr_numbers: list[int],
    pr_number_current: int | None,
) -> str:
    items = [
        f"- {' ' if pr_number == pr_number_current else ''}#{pr_number}"
        for pr_number in pr_numbers
    ]
    suffix = (
        BODY_SUFFIX_TITLE + "\n" + "\n".join(items) + "\n\n" + BODY_SUFFIX_FOOTER + "\n"
    )
    if re.search(BODY_SUFFIX_RE, body, flags=re.M | re.I | re.X):
        return re.sub(BODY_SUFFIX_RE, lambda _: suffix, body, flags=re.M | re.I | re.X)
    else:
        return body.rstrip() + "\n\n" + suffix


#
# Builds a body from an injected text.
#
def body_build_from_injected_text(
    *,
    injected_text: AiInjectedText,
) -> str:
    return re.sub(
        r"\s*" + re.escape(AI_PLACEHOLDER) + r"\s*",
        lambda _: f"\n\n{injected_text.text}\n\n",
        injected_text.template_with_placeholder,
    )


#
# Finds a previously known AI-generated snippet in the body and replaces it with
# AI_PLACEHOLDER marker, then returns an AiInjectedText object. It's an inverse
# of body_build_from_injected_text() basically.
#
def body_convert_to_injected_text(
    *,
    body: str,
    ai_generated_snippets: list[str],
) -> AiInjectedText | None:
    body = body.replace("\r", "")
    for snippet in ai_generated_snippets:
        pr_template = body.replace(snippet, AI_PLACEHOLDER)
        if pr_template != body:
            return AiInjectedText(
                template_with_placeholder=pr_template,
                text=snippet,
            )
    return None


#
# Returns an AI hash comment.
#
def ai_hash_build(hash: str) -> str:
    return re.sub(r"(?=-->)", lambda _: f":{hash}", AI_PLACEHOLDER)


#
# Extracts a hash from a body if it has an AI comment.
#
def ai_hash_extract(body: str) -> str | None:
    return m.group(1) if (m := re.search(ai_hash_build("([a-f0-9]+)"), body)) else None


#
# Hashes a text, but keeps it recognizable. Seems like in some shells, we can't
# have env variables with arbitrary characters, so we have to normalize
# (otherwise, some tests fail in GitHub Actions).
#
def hashify(text: str) -> str:
    return (
        re.sub(r"^_+|_+$", "", re.sub(r"[^A-Za-z0-9]+", "_", text))
        + "_"
        + short_hash(text)
    )


#
# Returns a short hash of a stripped text.
#
def short_hash(text: str) -> str:
    return hashlib.sha256(text.strip().encode()).hexdigest()[0:8]


#
# Parses a version string into a tuple of integers.
#
def parse_version(v: str) -> tuple[int, ...]:
    return tuple(map(int, (v.split("."))))


#
# Runs a function in a separate thread. To wait for its termination and get its
# return value, call wait() method. If the function raises an exception, it will
# be re-raised in the calling thread.
#
class Task(Generic[T]):
    _result: T
    _error: BaseException | None = None
    _thread: Thread

    def __init__(self, target: Callable[..., T], *args: ..., **kwargs: ...):
        def target_wrapper():
            try:
                self._result = target(*args, **kwargs)
            except BaseException as e:
                self._error = e

        self._thread = Thread(target=target_wrapper)
        self._thread.start()

    def wait(self) -> T:
        self._thread.join()
        if self._error:
            raise self._error
        return self._result


#
# If raised, the stacktrace is not shown.
#
class UserException(Exception):
    pass


if __name__ == "__main__":
    try:
        sys.exit(Main().run())
    except KeyboardInterrupt:
        signal.signal(signal.SIGINT, signal.SIG_DFL)
        os.kill(os.getpid(), signal.SIGINT)
    except UserException as e:
        print(re.sub(r"^", " ", str(e), flags=re.M))
        sys.exit(1)
    except CalledProcessError as e:
        print(
            f'Command "{shlex.join(e.cmd)}" returned status {e.returncode}.'
            + (f"\n{e.stdout}" if e.stdout else "")
            + (f"\n{e.stderr}" if e.stderr else ""),
            file=sys.stderr,
        )
        sys.exit(2)
